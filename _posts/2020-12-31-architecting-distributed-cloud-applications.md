---
layout:     post
title:      "Architecting Distributed Cloud Applications"
subtitle:   "分布式云服务的架构设计"
date:       2020-12-31 12:00:00
author:     "Baymax"
header-img: "img/post-bg-agt-9.jpg"
catalog: true
tags:
    - Architect
    - Project
---

> 本篇主要内容来自于油管系列视频：[Architecting Distributed Cloud Applications](https://www.youtube.com/playlist?list=PL9XzOCngAkqs0Q8ZRdafnSYExKQurZrBY)  
> 主要包含：基础认知、微服务、容器、网络、消息、服务更新、领导选举、数据存储等内容

## 分布式云服务

### 基础认知

#### 为什么要使用云服务

| 特点 | 过去 | 现在 |
| --- | --- | --- |
| 客户端 | 企业/内网 | 公共/互联网 |
| 业务量需求 | 稳定（少量） | 动态（少量->大量） |
| 数据库 | 单节点 | 多节点 |
| 运维手段 | 人力（贵） | 自动化（便宜） |
| 物理机 | 少量昂贵且专用的服务器 | 大量廉价的普通机型 |
| 故障 | 极少发生 | 非常容易发生 |
| 服务器掉线 | 灾难性的影响 | 正常（不是啥大事） |

上面几点主要列举了计算机软件的一些变化，举例说明：

1. 现代的系统需要面对成千上万的用户，并且在服务设计之初就需要考虑后面的业务量的增长，因此从一开始就需要为增加吞吐量留下余地
2. 由于用户可能遍布全球，数据库会从单节点或者私有化部署转到云上，变成多地多节点部署
3. 针对机器、网络、升级、部署等运维工作，由于云服务的存在，许多也可以进行自动化，不需要人工参与
4. 当然这一切都是有代价的，以往会默认机器极少出现掉线、崩溃的情况（尤其硬件不会轻易挂掉），但是现在必须把这些当成常见的情况来处理

由于上面的一些要求和变化，构建服务的时候，也必须和以往有不同的设计。比如在处理异常的时候，不能再简单地catch异常而不进行处理，需要允许服务崩溃，但是要提供一整套应对措施，让服务可以自动重启。

#### 拥抱异常

一些可能会导致服务异常的情况：

1. 代码中没有catch的异常，导致服务宕机
2. 扩缩容导致没有足够的机器，或者请求到了没有完全启动的机器上
3. 服务升级时，代码没有完全兼容上个版本，请求被打到了两种不同的代码上，导致异常
4. 服务之间进行协作时可能会导致异常，比如负载均衡和另一个服务在某种情况下产生的代码bug等
5. 物理打击，比如断网、断电；外部攻击，数据库被攻击等

既然异常是一定会发生的，那么不如拥抱异常，假设服务宕机是经常发生的，为了更好地应对这种情况，需要：

1. 编写的代码不能依赖硬件设施，在通用硬件环境下都需要可以运行
2. 当遭遇异常情况的时候，可以自动进行重试而不影响原有业务
2. 使用框架来解决单点故障，让服务在失败的时候可以自动重启
3. 使用备用服务器来保障服务的可用性，当某些服务宕机的时候，这些备用服务器可以立刻顶上

#### 资源编排服务（orchestrators）

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/orchestrators.jpg)

资源编排服务指的是用于统筹管理一个计算机集群的组件或者框架，这里的计算机可以是物理机，也可以是虚拟机。资源编排服务需要完成以下工作：

1. 管理机器的生命周期，比如新建、初始化、销毁等
2. 管理网络，让一个组内的网络可以相互访问，而不在一个组的即使是同一台物理机上，也无法访问
3. 提供监控系统，比如cpu、内存用量等，有的框架甚至可以提供监控相关的接口，用于自动化处理
4. 提供代码升级、部署、服务扩缩容服务，比如滚动升级代码、将两个非常消耗资源的服务从物理机上分割等

随着时间的推移，资源编排服务和业务代码逐渐解耦，也因此在大部分情况下，写代码并不需要去考虑资源编排服务的内容。另外，这样也导致了开发和部署会在完全不同的环境中，代码会在本地电脑开发，然后通过资源编排服务部署到集群中。

#### 地域（regions）和可用区（availability zones）

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/regions.jpg)

由于全球化服务的需求，在不同的地方往往需要建立不同的地区数据中心，因此地域是一个地点概念。可用区指的是在同一个地域内，会再独立假设电力、网络等设备，保障可用区之间相互独立，这样就不会形成区域单点故障。另外在同一个地域的可用区，会使用高速网络连接，这样就保证了在一个地域内的服务响应速度。

举例来说，一个外部请求过来，最终要打到某一个具体的服务上。通常服务会假设在虚拟机（VM）上，虚拟机会运行在某个特定的物理机（PC）上，物理机又会在某一个机架（rack）上，一组机架可能会属于同一个可用区，多个可用区组成一个地域，然后所有的地域都在地球上。

因此，某个级别的东西挂了之后，它以及它的下层都不再能提供服务。从这个角度来说，地球就是最大的单点故障，如果某天地球boom了，那么所有的服务都不复存在了。

这里会引发容错率和延迟之间的取舍，比如同样的数据如果放在不同的虚拟机，但是在同一台物理机上，它们共享数据的时间成本很低，不过只要这台物理机挂了，那么这些数据就丢失了；同理，如果放在不同的物理机，但在同一个可用区，那么数据同步的成本就更高了，与此同时容错级别就提高到了可用区级别。

### 微服务（micro-services）

#### 微服务示例

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/microservices-1.jpg)

微服务改造是说：将一个大型复杂软件拆分成许多相互协作的小型服务，上图就是微服务架构的一个示例，图中由三个服务构成：web服务、库存服务和订单服务，由于不同服务的吞吐量要求不同，它们启动的虚拟机实例数量也不同。

另外微服务架构要求微服务之间不应该公用数据库，理由是一般来说每个服务会要求维护自己的数据库schema版本，甚至有的服务使用sql数据库，而有的使用nosql数据库。这样进行数据迁移和升级的时候，每个服务仅需要考虑自身的数据需求即可，不存在外部依赖。

#### 迁移到微服务的好处

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/microservices-2.jpg)

在服务建立初期，一般都使用一个巨大的服务来实现所有的功能，那么在什么时候应该考虑迁移到微服务架构呢？一般而言，微服务相对与大型复杂软件会有几个优势：

- 不同服务可能会有不同的吞吐量和延迟要求，划分成微服务之后，可以对单个服务进行扩缩容操作
- 对于不同的服务，可以使用完全不同的技术栈（比如一个使用node.js技术栈，另一个使用java技术栈）
- 不同的客户端都需要使用同样的功能，如果使用微服务的话就不需要在每个服务中都实现一样的代码
- 希望在某一个技术栈中使用不同版本的类库，比如某些服务使用v1版本，某些服务使用v2版本的服务

同时，微服务也不应该被神话，有很多好处并不是微服务带来的，比如：

1. 微服务让代码跟简洁易读：这应该是通过oop或者其他的编码方式带来的，而不是微服务的好处
2. 微服务让一个服务的异常不影响另一个服务：这并不实际，比如用户认证服务异常了，那么所有与用户身份相关的服务都将失败，因为它们不知道用户的身份
3. 其他一些微服务不具备的规范：比如更快迭代、更少的bug率之类的需求，微服务都无法满足

#### SLA服务可用性的计算

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/microservices-3.jpg)

在改造成微服务之后，对应SLA理论上是有不利影响的，参照上面的表格，分别假设每个服务的SLA都是 $99.99\%$ 和 $99.999\%$ ，那么多个服务相互依赖的情况下，整体的SLA就会下降，最后的结果是如果有n个有依赖关系的服务，那么最终的SLA分别是 $99.99^n\%$ 和 $99.999^n\%$ 。

#### 自动扩缩容

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/microservices-4.jpg)

自动扩缩容指的是根据服务的负载不同，自动增加/减少虚拟机实例的数量，一般会用于自动扩缩容的几种技术：

1. 如果在客户端和服务之间有一个消息队列的话，可以监控消息队列的长度，如果队列增长，那么扩容，如果队列空闲那么缩容
2. 如果在客户端和服务之间是一个负载均衡的话，那么可以监控各个虚拟机的cpu、内存用量情况，如果都处于繁忙状态，那么扩容，如果相对空闲那么缩容
3. 根据设定好的schedule来执行，比如在节日、日常和周末使用不同的虚拟机数量，但是这种情况需要额外注意，因为调整的数量可能和预期非常不符

#### 12条标准

引用自网站：[The Twelve Factors](https://12factor.net/)，这12条标准是构建微服务的一个参考，在这里并不会深入每一条标准讲述，而只是介绍这12条标准的思想，为构建微服务作为参考：

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/microservices-5.jpg)

第一条：使用统一的代码仓库，并且不同的服务之间不分享代码。  
第二条：每一个服务在部署时会自动带上自己依赖的包，也就是说单个服务不应该依赖外部包，这保障了服务的独立性，会对开发、测试和部署都带来很多好处。  
第三条：不要将配置文件放到代码里，而是读取环境变量。  
第四条：在处理另一个服务无响应的异常时，要使用有容错率的方案，不能让当前服务轻易崩溃。  
第五条：严格区分build、release和run，这一条对三者做出了不同的定义：build——将代码和依赖一起打包并确定版本；release——将打好的包和环境变量绑定，形成镜像；run——把镜像放到执行环境中运行。  

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/microservices-6.jpg)

第六条：微服务由一个或多个无状态的进程组成，它们之间也不应该共享数据，这一条是为了保障服务可以被随时停掉或者启动，而不会产生副作用。  
第七条：应该监控特定的端口号，不应该监控特定的IP地址。  
第八条：非并发情况使用单进程，并发情况使用多进程，尽量避免多线程编程，因为多线程非常复杂也极容易出错。  
第九条：微服务进程应该可以快速重启，避免使用过于复杂的初始化和销毁代码。  
第十条：保持开发、测试和发布环境是相似的，从而减少环境问题。  
第十一条：把日志打到stdout中，也就是说在开发时打印到屏幕上，在正式环境中重定向到另一个文件中——也就是keep logging simple。  
第十二条：所有需要发布或者执行的脚本，都应该写道脚本中，而不是手动登录线上机器修改，并且要在所有的环境中测试这些脚本。  

**标准总结**

如果深入理解这12条标准，其实就之有一条：make things simple，让开发、测试、部署的每一步都变得简单，微服务也应该是轻量级的，这样就方便每个步骤的执行，并且每一步都是代码层面可见的，没有附加的黑魔法。

### 容器（container）

#### 镜像和容器

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/containers-1.jpg)

镜像：一个镜像包含了某个版本的服务以及它所有的依赖类库，并且它是不可变的。
容器：独立运行镜像的环境，即可以运行并保障容器之间是相互隔离的。

使用镜像最大的好处在于每一个版本的服务都可以放置于镜像中，并且不同的镜像之间是相互独立的，因此可以方便的切换镜像的版本，从而进行服务的升级和降级。另外，多个镜像可以运行在同一台物理机或者虚拟机上，从而增加了更多的灵活性。

#### 隔离级别的取舍

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/containers-2.jpg)

上面的图片将隔离级别分为三种：硬件级别、操作系统级别和资源级别。其中隔离级别最高的是物理机，不同物理机之间所有的东西都不共享，最低的是进程级别，它共享了所有内容，容器属于资源级别的隔离。

由于视频作者来自微软，因此特别介绍了微软的Hyper-V容器，它比虚拟机更轻量但是使用了独立的操作系统底层。在这之后微软又推出了WSL（Windows Subsystem for Linux），目前我也在使用WSL进行后端开发，并且docker也使用了WSL作为底层引擎。

#### 容器启动的流程

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/containers-3.jpg)

这里使用了Docker作为资源编排服务的例子，整个启动一个容器的过程分为几个步骤：

1. 从客户端发起命令：docker run Svc-A:v1
2. 服务端根据命令在本地镜像仓库中寻找 Svc-A:v1 对应的镜像，如果本地没有找到的话，则会去外部的镜像仓库中拉取镜像
3. 服务端找到镜像之后，会根据命令启动一个容器，并将 Svc-A:v1 的镜像载入到容器中运行

就像前面介绍的那样，资源编排服务还可以提供资源监控，限制容器资源用量的功能。

#### 持续集成、持续交付、持续部署

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/containers-4.jpg)

现代的开发流程中，经常会遇到持续集成、持续交付和持续部署这几个词，这一整套流程大概是这样：

1. 首先需要一个公共的代码仓库和镜像仓库，开发人员将代码push到这个代码仓库中
2. 触发持续集成，持续集成的服务检测到代码更新，自动打包并且生成镜像，并且将镜像push到镜像仓库中
3. 触发持续交付，持续交付的服务开启一个小的集群，运行所有的单元测试和集成测试用例
4. 测试通过之后，再进入预发布环境测试，对于toC的服务而言，这可能是一部分线上机器和真实用户
5. 最后是持续部署，这里一般不是自动执行的，因为会影响到所有的用户，必须确保万无一失才能执行

上面的任意一个步骤都有可能失败，并且只要任意一个步骤失败了，都必须检查失败的原因，修复之后重新执行整体流程。

## 网络（networking）

### 基础认知

#### 常见的8种误区

| 误区 | 实际上 |
| --- | --- |
| 网络是可靠的 | 必须在代码里捕获网络连接异常并重试 |
| 请求和本地一样没有延时 | 必须减少远程请求的调用次数 |
| 不需要考虑网络带宽 | 每次请求都需要考虑如何减少传递的数据 |
| 网络是安全的 | 在某些情况下必须使用加密传输 |
| 网络拓扑是不变的 | 延迟、带宽机器ip都会变化 |
| 有超级管理员可以管理网络 | 服务的配置可能是分散的，配置变更会导致服务不可访问 |
| 网络耗时可以不考虑 | 网络耗时会在整个请求时常中占到一定的比例 |
| 网络状态是稳定的 | 延迟、带宽等各种网络状态都随时可能会变化 |

其中的关键点在于之前所提到的，云端代码会运行在虚拟机上，而虚拟机又会在不同的实体机和可用区甚至不同的地域，因此网络的本身就是不可靠的，表格中的某些内容可能需要具体说明：

1. 由于服务之间的通信最后还是会落到某个物理连接上，而这些连接是共享的，因此服务之间减少传递的数据不仅仅是为当前服务考虑，也是为其他服务考虑
2. 对于集群来说，如果创建了一个内网环境，那么外部是无法访问内部的，但是由于第三方插件和第三方库的存在，内部的流量有可能被监听，那么所有的服务都不安全了
3. 网络拓扑会由于资源编排服务的自动调整而导致变化，比如某些虚拟机会由系统自动回收，这样它的ip地址就不可达了

#### 服务的注册与发现

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/networking-1.jpg)

在以前，我们的服务总是在某台计算机上监听某个端口，因此通过ip和port就可以打到对应的服务上，但是在云环境下，这样的操作会复杂的多：

- 一台实体机会启动许多的虚拟机，每个虚拟机中又可能启动多个服务，每个服务都会监听自己的端口
- 许多服务可能监听同样的端口，此时就需要一些额外的技术来保证服务能够正常运行
- 可能需要使用：路由表、SNAT/DNAT、甚至直接修改客户端代码，但是最终的结果就是不停的加入新的黑魔法

再加上网络还有网卡、路由、DNS等等现有系统的掣肘，服务在网络上的注册、发现与通信远比想象中的复杂。另外，在这些的基础之上，还需要提供服务的自动扩容、缩容，服务的失败自动重启等功能。

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/networking-2.jpg)

好消息是，现在可以通过反向代理来解决上面提到的这些问题。客户端可以记录反向代理的地址，并且将所有的请求打到反向代理上，再由反向代理负责转发，但是由于所有的请求都要经过反向代理，就增加了系统延迟，这个方案的本质是用系统延迟的代价换来了更好的可扩展性。

#### 正向代理和反向代理

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/networking-3.jpg)

**正向代理（Forward Proxies）**

正向代理是客户端的代理，有的时候也直接说是代理，某些公司会提供一个统一的代理，这样公司内的所有机器流量都会经过这个代理，正向代理一般用于：

- 过滤用户请求，比如屏蔽某些网站，禁止访问
- 提供缓存服务、日志服务、监控服务
- 对服务端匿名访问（以代理的ip访问服务端），这个功能爬虫防止封禁会用的比较多

**反向代理（Reverse Proxies）**

反向代理是服务器端的代理，一般而言会提供以下功能：

- 向客户端提供一个稳定的入口，并映射到所有的服务上
- 负责负载均衡、服务选择或者A/B测试
- 提供跳板机服务、缓存服务、用户认证服务、流量计费服务
- 防御DDos攻击或者减少其影响

#### 集群内DNS服务

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/networking-4.jpg)

上图是一个使用了集群内DNS服务的例子，一个客户端请求进入集群，首先来到了负载均衡，负载均衡转交到了一台web服务器上，然后需要和调用库存服务，但是由于资源编排的存在，库存服务的入口随时可能发生变化，因此引入集群内DNS服务，它的本质是一张映射表，把一个服务名称和它对应的地址映射在一起。有了DNS之后，就可以找到库存服务的反向代理地址了，然后请求就会由这个反向代理，打到某一个具体的库存服务实例上，由这个实例处理这一次请求的内容。

在上述的流程中，有几个需要考虑的问题，主要还是和请求失败/报错有关：

1. 当web服务器的请求打到库存服务的时候，处理请求的实例可能会报错崩溃，此时DNS服务就需要知道这个变化，并且让这个实例从DNS中移除，并且web服务器要重试该请求
2. 还有一种可能是库存服务正常处理了，但是返回的时候web服务器已经宕机，此时客户端的这次请求就已经直接失败了，客户端需要重试这次请求
3. 另外上面的设计中，DNS本身是一个单点故障，它挂了会导致整个集群服务不可用；各个服务的反向代理也是一个单点故障，只要挂了它所代理的服务也就不可用了，因此这些服务也虽好可以设置多台机器来保障服务的可用性

#### 服务状态探针

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/networking-5.jpg)

如上图所示，对于某些反向代理来说（比如负载均衡），每个实例的状态是很重要的，为了避免请求打到压力过大甚至宕机的实例上，负载均衡必须知道每一个实例的状态，从而更好地分发请求。一般的做法是每个实例都暴露一个专门用于检查健康状态的接口，由负载均衡定时检查这些接口的状态：

- 如果延迟过大，或者有503这样的错误，就需要减少发往该实例的请求
- 如果直接没有响应，就需要将这个实例从可用的列表中删除，并向资源编排服务申请新的资源

### 远程函数调用

#### 从本地函数迁移到远程函数

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/networking-6.jpg)

在把一个大型复杂软件转化成众多微服务的过程中，需要把很多进程内部方法调用转化成远程函数调用，即通过网络调用另一个微服务的函数，在这过程中，许多事情都会发生变化：

1. 首先会丢失很多语言特性，原先的编译时异常会变成运行时异常，单元测试也不能像以前一样检查函数内部逻辑
2. 在调用的时候，会涉及到序列化和反序列化的过程，这就要求传递的数据必须支持序列化，另外序列化和反序列化的耗时也是一个额外的开销
3. 相比进程内调用，远程调用本质上是不可靠的，比如：会受到延迟和掉线等网络的问题影响、有可能被监听或者被未授权的服务调用、报错的调用栈会非常难追踪等，这些都是需要做额外处理的

#### API版本控制

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/networking-7.jpg)

本质上来说，微服务是没有API升级的说法的，因为一旦API已经暴露给其他服务，任何的升级都必须保持向前兼容。比较推荐的做法是在调用的时候指明需要调用的API版本号，另外在做API版本变更的时候，一般都是需求、功能变化或者代码重构等情况导致，在这些情况发生的时候，必须和客户端做好沟通，指导他们更新需要调用的API版本以及明确新版API发生的变化，避免出现服务不可用的情况。

#### API设计建议

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/networking-8.jpg)

设计一个微服务API时，这里给出了几条基本的建议：

1. 定义明确的、跨语言可用的API，不要依赖于某一种语言特性
2. 使用openapi或者swagger之类的API定义工具，自动生成各语言的客户端代码
3. 使用跨语言的数据结构定义框架，比如JSON/XML、Thrift等，另外可以考虑在数据结构中加入版本号

#### RPC封装的缺陷

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/networking-9.jpg)

需要小心封装后的远程调用，现实中已经存在了许多这样的标准和类库，根据之前对于网络的介绍，这些封装标准大多数时候都不能解决以下问题：

- 处理网络异常情况，比如：重试、超时、无限递归调用等
- 处理语言之间类型支持不同的问题
- 处理API版本控制问题
- 处理调用时加密和认证问题
- 处理日志记录问题

#### Exactly-once语义

一般来说，在执行一个函数或者处理数据时，会有三种语义：

1. 最多一次（At-most-once）
2. 至少一次（At-least-once）
3. 精确一次（Exactly-once）

其中最常用也是最困难的就是最后一种：精确一次（Exactly-once），这种语义要求调用函数最后的效果是正好调用一次的效果，这种语义并不是说函数仅调用一次并且保证不在任何环节出问题，而是包含了客户端和服务端两者的结合：

- 对于客户端来说：在分布式服务的环境中，是充满了失败和报错的，因此在遇到错误时必须重新请求，直到调用成功或者达到最大重试次数
- 对于服务端来说：由于客户端会不停地尝试，并且由于资源编排服务的存在，重试的请求很可能会打到不同的服务实例上，因此服务端必须保证该调用是幂等的，即多次调用都可以正确执行并且不会产生副作用

Exactly-once语义的实现方式就是客户端重试+服务端幂等的结合。

#### 常见的幂等处理

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/networking-10.jpg)

一般来说常见的操作就是增删改查，其中除了新增以外，其他的方法都有比较好的幂等方案：

- 查询，如果不做额外操作的话天然是幂等的
- 修改，不管如何修改，最后那一次的修改会被记录
- 删除，如果对应元素已经被删除，那么不做任何修改

问题在于新增数据的时候，容易出现多次请求导致新增多条数据的情况，因此引入**幂等设计模式**，一般包含三个步骤：

1. 客户端：获取一个唯一ID，可以由客户端生成或者客户端请求服务端获取
2. 客户端：发送该ID和需要执行的函数到服务端
3. 服务端：检查ID是否存在、执行函数、将ID记录到日志中（这三个操作必须是原子的）

另外有两个需要关注的内容，一是服务端的三个操作必须是原子的，否则也无法保证幂等性；另外一个是不要忘记定期清理服务端的ID记录，否则会导致记录无限增长。

## 消息（messaging）

### 基础认知

#### 为什么要使用消息模式

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/messaging-1.jpg)

原有请求-响应模式在分布式环境下有一些先天不足：请求可能会打到挂起或者繁忙的服务实例、当请求完成之后，客户端也可能会宕机或者被缩容。如果在系统中遇到了这些问题，并且难以解决的话，可以考虑切换为消息模式，消息模式会有以下好处：

- 效率更高：客户端不需要等待返回；服务端性能更强的实例自动就会拉取更多任务，因此负载均衡效果也更优秀；客户端服务端不再需要直接交互，和消息队列交互即可
- 容错率更强：服务端处理失败之后可以直接由另一个实例再进行处理，这里同样要求服务端要保持处理时的幂等性
- 扩缩容更容易：直接监控消息队列的长度即可决定是否要进行扩缩容处理

#### 消息队列的使用

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/messaging-2.jpg)

上图是一个使用了消息队列的分布式服务案例，可以通过这个案例来了解消息队列的使用方式，从客户端发起一个web请求开始：

1. 客户端发起请求后，由负载均衡分发到某个web服务上，这一步还是一个http请求，再打到某一个web服务实例上，假设是WS1
2. web服务调用服务A，这里改为发送一条消息到服务A的消息队列，服务A的三个实例都可以从消息队列拿到消息并处理
3. 服务A调用服务B，这里改为发送一条消息到服务B的消息队列，服务B的两个实例都可以从消息队列拿到消息并处理
4. 服务B处理完的结果再返回给服务A，同样发消息到服务A的消息队列处理，此时可能和上一次处理的并不是同一个服务A实例
5. 最后服务A将处理完的结果发送给WS1的消息队列，然后WS1接收到返回的数据，并返回给客户端

上面的设计中，只有web服务是每一个实例都有一条消息队列，这是因为web服务对接的客户端可能需要同步返回结果，此时就需要等待消息队列中出现当前实例发出请求的返回消息，让每个实例绑定自己的消息队列，就可以在扩缩容和web服务端报错的时候不再需要做额外的清理操作。

相比原有的请求模型，消息队列的使用在效率上有以下几个优势：

- 服务A不需要再等待服务B的返回，只需要将请求发送到服务B的消息队列中即可，节约了等待的时间
- 如果服务A本身不需要对服务B的返回数据做处理，那么服务B就可以直接将结果发送到WS1的消息队列中，WS1就可以直接返回给客户端，再一次节约了开销
- 在微服务的层级也不需要重试请求的代码了，因为直接交给了另一个消息队列，会由另一个微服务保证它能够被正确处理，当然了，web服务还是需要重试机制的

#### 高容错的消息处理

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/messaging-3.jpg)

可以发现，在之前对于消息模式的介绍中，都要求每个微服务本身对消息可以进行高容错的处理，这里给出了一种高容错的消息处理方案：

- 给每一条消息新增一个字段：DequeueCount，用于标记该消息已经被出队列几次
- 当有一个实例拉取一条消息时，将该消息的DequeueCount + 1，并且隐藏该消息一定时间（类似于重试请求等待的超时时间）
- 拿到消息的服务，检查消息的DequeueCount字段，如果大于某阈值，那么log下这条消息和失败信息，并且记录消息处理结果为失败；当服务处理完消息之后，将该消息从消息队列中删除

#### 额外特性

![](/img/in-post/2020-12-31-architecting-distributed-cloud-applications/messaging-4.jpg)

一般而言，消息模式除了上面提到的好处之外，还可以支持一些额外特性：

- 消息可以被发送给多个订阅者，通过这个特性可以实现广播或者异步任务处理
- 消息超时时间，在经过这个时间之后，消息会自动销毁，这可以避免在极端情况下消息队列无限增长
- 某些消息队列可以在运行中修改消息的隐藏时间，这给消息处理提供了很多便利，比如：可以设置一个很短的消息隐藏时间来减少延迟，但是在处理过程中周期性地增加隐藏时间，避免两个实例同时处理；另外，可以通过将隐藏时间设置成大于消息自动销毁的时间，从而实现At-most-once语义

